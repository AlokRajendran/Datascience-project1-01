{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OpLuiwZnYRpV"
   },
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-Y1oMpE1qD1"
   },
   "source": [
    "**BOW,TF_IDF,WORD2VEC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QakneJpgY0MB"
   },
   "source": [
    "**Bag of words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words(BoW) Model\n",
    "\n",
    "We cannot pass text directly to train our models in Natural Language Processing, thus we need to convert it into numbers, which machine can understand and can perform the required modelling on it. The Bag of Words(BoW) model is a fundamental(and old) way of doing this.\n",
    "\n",
    "The BoW model is very simple as it discards all the information and order of the text and just considers the occurrences of the word, in short it converts a sentence or a paragraph into a bag of words with no meaning. It converts the documents to a fixed-length vector of numbers.\n",
    "\n",
    "A unique number is assigned to each word(generally index of an array) along with the count representing the number of occurence of that word. This is the encoding of the words, in which we are focusing on the representation of the word and not on the order of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PjEik49gaP2Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer tokenizes(tokenization means breaking down a sentence or paragraph or any text into words) the text along with performing very basic preprocessing like removing the punctuation marks, converting all the words to lowercase, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "lOnENbjQdG3J",
    "outputId": "9a6a2314-c69f-4725-935d-424d0ef570cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "#each word is given a unique code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "5COvgBj_hpBX",
    "outputId": "5fe7e979-ec11-4e3b-a402-7ecb7ff078a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'machine': 7, 'learning': 6, 'is': 4, 'great': 2, 'natural': 8, 'language': 5, 'processing': 9, 'complex': 0, 'field': 1, 'used': 10, 'in': 3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Machine learning is great \",\"Natural Language Processing is a complex field\",\n",
    "\"Natural Language Processing is used in machine learning\"]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer() \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(sentences)\n",
    "print(vectorizer.vocabulary_)\n",
    "vectorizer.transform([\"Machine learning is great\",\"Natural Language Processing is a complex field\",\n",
    "\"Natural Language Processing is used in machine learning\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqBiJRONFR8D"
   },
   "source": [
    "**Drawbacks of using a Bag-of-Words (BoW) Model**\n",
    "\n",
    "In the above example, we can have vectors of length 11. However, we start facing issues when we come across new sentences:\n",
    "\n",
    "If the new sentences contain new words, then our vocabulary size would increase and thereby, the length of the vectors would increase too.\n",
    "\n",
    "Additionally, the vectors would also contain many 0s, thereby resulting in a sparse matrix (which is what we would like to avoid)\n",
    "\n",
    "We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEAqZUrfDepC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPH7FNC3jykY"
   },
   "source": [
    "**TF-IDF**\n",
    "\n",
    "**Frequency**: This summarizes how often a given word appears within a document.\n",
    "\n",
    "**Document Frequency**: This downscales words that appear a lot across documents.\n",
    "\n",
    "**Inverse Document Frequency (IDF):** is a weight indicating how commonly a word is used. The more frequent its usage across documents, the lower its score. The lower the score, the less important the word becomes.\n",
    "\n",
    "For example, the word the appears in almost all English texts and would thus have a very low IDF score as it carries very little “topic” information. In contrast, if you take the word coffee, while it is common, it’s not used as widely as the word the. Thus, coffee would have a higher IDF score than the.\n",
    "\n",
    "**TF-IDF**: is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YeOSJDxmqPUj"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The car is driven on the road.\",\"The truck is driven on the highway\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "wQN0VCjgp7dl",
    "outputId": "3a1cbe3c-ff55-4e21-e8e9-c74e0cdcc87c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "lY21KPcUUEhz",
    "outputId": "97d8a5b5-55a7-4cd4-f0c1-e006edb4a0fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42471719 0.30218978 0.         0.30218978 0.30218978 0.42471719\n",
      "  0.60437955 0.        ]\n",
      " [0.         0.30218978 0.42471719 0.30218978 0.30218978 0.\n",
      "  0.60437955 0.42471719]]\n"
     ]
    }
   ],
   "source": [
    "X =vectorizer.fit_transform(text).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "vkqchdH5r17B",
    "outputId": "ccb8e319-6b85-4e1f-a179-a2795cd32455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.40546511 1.         1.40546511 1.         1.         1.40546511\n",
      " 1.         1.40546511]\n"
     ]
    }
   ],
   "source": [
    "#Focus on IDF VALUES\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lI3Eu9JkhFin"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "OLdIWgJeapbw",
    "outputId": "dc03e928-7515-472e-e7d6-0d316bc0b450"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91956\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "GF7QdQ0yauA9",
    "outputId": "a0a6efc1-1c7b-46f9-9803-62243a962f67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91956\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "wU7EfCXVbCaZ",
    "outputId": "f609bd57-66bb-49cc-d43e-c1372d2bffb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91956\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0Jhqm5Gp-4A"
   },
   "source": [
    "**End Notes**\n",
    "\n",
    "Let me summarize what we’ve covered in the article:\n",
    "\n",
    "Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.\n",
    "Bag of Words vectors are easy to interpret. However, TF-IDF usually performs better in machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cC2Zv9ommPdL"
   },
   "source": [
    "**word2vec**\n",
    "\n",
    "In both Bow and TF-IDF approach semantic information is not stored.\n",
    "\n",
    "TF-IDF gives importance to uncommon words\n",
    "\n",
    "There is defenitely chance of overfitting\n",
    "\n",
    "\n",
    "In word2vec model each word is basically represented as a vector of 32 or more dimension.\n",
    "\n",
    "Here, the semantic information and the relation between different words is also preserved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\91956\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\91956\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\91956\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\91956\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\91956\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "L5MuF-orpoaj"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "#anaconda !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tdG5o1Pkp3UU"
   },
   "outputs": [],
   "source": [
    "\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "GDx1gL7GqEdm",
    "outputId": "a9c5725a-eb04-4309-ff95-57186e203796"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have three visions for india. in 3000 years of our history, people from all over the world have come and invaded us, captured our lands, conquered our minds. from alexander onwards, the greeks, the turks, the moguls, the portuguese, the british, the french, the dutch, all of them came and looted us, took over what was ours. yet we have not done this to any other nation. we have not conquered anyone. we have not grabbed their land, their culture, their history and tried to enforce our way of life on them. why? because we respect the freedom of others.that is why my first vision is that of freedom. i believe that india got its first vision of this in 1857, when we started the war of independence. it is this freedom that we must protect and nurture and build on. if we are not free, no one will respect us. my second vision for india’s development. for fifty years we have been a developing nation. it is time we see ourselves as a developed nation. we are among the top 5 nations of the world in terms of gdp. we have a 10 percent growth rate in most areas. our poverty levels are falling. our achievements are being globally recognised today. yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. isn’t this incorrect? i have a third vision. india must stand up to the world. because i believe that unless india stands up to the world, no one will respect us. only strength respects strength. we must be strong not only as a military power but also as an economic power. both must go hand-in-hand. my good fortune was to have worked with three great minds. dr. vikram sarabhai of the dept. of space, professor satish dhawan, who succeeded him and dr. brahm prakash, father of nuclear material. i was lucky to have worked with all three of them closely and consider this the great opportunity of my life. i see four milestones in my career'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NnCPllBrqM-W",
    "outputId": "b6a2980e-c79a-4af3-9fb2-f9b08977f4bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'have', 'three', 'visions', 'for', 'india', '.'],\n",
       " ['in',\n",
       "  '3000',\n",
       "  'years',\n",
       "  'of',\n",
       "  'our',\n",
       "  'history',\n",
       "  ',',\n",
       "  'people',\n",
       "  'from',\n",
       "  'all',\n",
       "  'over',\n",
       "  'the',\n",
       "  'world',\n",
       "  'have',\n",
       "  'come',\n",
       "  'and',\n",
       "  'invaded',\n",
       "  'us',\n",
       "  ',',\n",
       "  'captured',\n",
       "  'our',\n",
       "  'lands',\n",
       "  ',',\n",
       "  'conquered',\n",
       "  'our',\n",
       "  'minds',\n",
       "  '.'],\n",
       " ['from',\n",
       "  'alexander',\n",
       "  'onwards',\n",
       "  ',',\n",
       "  'the',\n",
       "  'greeks',\n",
       "  ',',\n",
       "  'the',\n",
       "  'turks',\n",
       "  ',',\n",
       "  'the',\n",
       "  'moguls',\n",
       "  ',',\n",
       "  'the',\n",
       "  'portuguese',\n",
       "  ',',\n",
       "  'the',\n",
       "  'british',\n",
       "  ',',\n",
       "  'the',\n",
       "  'french',\n",
       "  ',',\n",
       "  'the',\n",
       "  'dutch',\n",
       "  ',',\n",
       "  'all',\n",
       "  'of',\n",
       "  'them',\n",
       "  'came',\n",
       "  'and',\n",
       "  'looted',\n",
       "  'us',\n",
       "  ',',\n",
       "  'took',\n",
       "  'over',\n",
       "  'what',\n",
       "  'was',\n",
       "  'ours',\n",
       "  '.'],\n",
       " ['yet',\n",
       "  'we',\n",
       "  'have',\n",
       "  'not',\n",
       "  'done',\n",
       "  'this',\n",
       "  'to',\n",
       "  'any',\n",
       "  'other',\n",
       "  'nation',\n",
       "  '.'],\n",
       " ['we', 'have', 'not', 'conquered', 'anyone', '.'],\n",
       " ['we',\n",
       "  'have',\n",
       "  'not',\n",
       "  'grabbed',\n",
       "  'their',\n",
       "  'land',\n",
       "  ',',\n",
       "  'their',\n",
       "  'culture',\n",
       "  ',',\n",
       "  'their',\n",
       "  'history',\n",
       "  'and',\n",
       "  'tried',\n",
       "  'to',\n",
       "  'enforce',\n",
       "  'our',\n",
       "  'way',\n",
       "  'of',\n",
       "  'life',\n",
       "  'on',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['why', '?'],\n",
       " ['because',\n",
       "  'we',\n",
       "  'respect',\n",
       "  'the',\n",
       "  'freedom',\n",
       "  'of',\n",
       "  'others.that',\n",
       "  'is',\n",
       "  'why',\n",
       "  'my',\n",
       "  'first',\n",
       "  'vision',\n",
       "  'is',\n",
       "  'that',\n",
       "  'of',\n",
       "  'freedom',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'believe',\n",
       "  'that',\n",
       "  'india',\n",
       "  'got',\n",
       "  'its',\n",
       "  'first',\n",
       "  'vision',\n",
       "  'of',\n",
       "  'this',\n",
       "  'in',\n",
       "  '1857',\n",
       "  ',',\n",
       "  'when',\n",
       "  'we',\n",
       "  'started',\n",
       "  'the',\n",
       "  'war',\n",
       "  'of',\n",
       "  'independence',\n",
       "  '.'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'this',\n",
       "  'freedom',\n",
       "  'that',\n",
       "  'we',\n",
       "  'must',\n",
       "  'protect',\n",
       "  'and',\n",
       "  'nurture',\n",
       "  'and',\n",
       "  'build',\n",
       "  'on',\n",
       "  '.'],\n",
       " ['if',\n",
       "  'we',\n",
       "  'are',\n",
       "  'not',\n",
       "  'free',\n",
       "  ',',\n",
       "  'no',\n",
       "  'one',\n",
       "  'will',\n",
       "  'respect',\n",
       "  'us',\n",
       "  '.'],\n",
       " ['my', 'second', 'vision', 'for', 'india', '’', 's', 'development', '.'],\n",
       " ['for',\n",
       "  'fifty',\n",
       "  'years',\n",
       "  'we',\n",
       "  'have',\n",
       "  'been',\n",
       "  'a',\n",
       "  'developing',\n",
       "  'nation',\n",
       "  '.'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'time',\n",
       "  'we',\n",
       "  'see',\n",
       "  'ourselves',\n",
       "  'as',\n",
       "  'a',\n",
       "  'developed',\n",
       "  'nation',\n",
       "  '.'],\n",
       " ['we',\n",
       "  'are',\n",
       "  'among',\n",
       "  'the',\n",
       "  'top',\n",
       "  '5',\n",
       "  'nations',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  'in',\n",
       "  'terms',\n",
       "  'of',\n",
       "  'gdp',\n",
       "  '.'],\n",
       " ['we',\n",
       "  'have',\n",
       "  'a',\n",
       "  '10',\n",
       "  'percent',\n",
       "  'growth',\n",
       "  'rate',\n",
       "  'in',\n",
       "  'most',\n",
       "  'areas',\n",
       "  '.'],\n",
       " ['our', 'poverty', 'levels', 'are', 'falling', '.'],\n",
       " ['our',\n",
       "  'achievements',\n",
       "  'are',\n",
       "  'being',\n",
       "  'globally',\n",
       "  'recognised',\n",
       "  'today',\n",
       "  '.'],\n",
       " ['yet',\n",
       "  'we',\n",
       "  'lack',\n",
       "  'the',\n",
       "  'self-confidence',\n",
       "  'to',\n",
       "  'see',\n",
       "  'ourselves',\n",
       "  'as',\n",
       "  'a',\n",
       "  'developed',\n",
       "  'nation',\n",
       "  ',',\n",
       "  'self-reliant',\n",
       "  'and',\n",
       "  'self-assured',\n",
       "  '.'],\n",
       " ['isn', '’', 't', 'this', 'incorrect', '?'],\n",
       " ['i', 'have', 'a', 'third', 'vision', '.'],\n",
       " ['india', 'must', 'stand', 'up', 'to', 'the', 'world', '.'],\n",
       " ['because',\n",
       "  'i',\n",
       "  'believe',\n",
       "  'that',\n",
       "  'unless',\n",
       "  'india',\n",
       "  'stands',\n",
       "  'up',\n",
       "  'to',\n",
       "  'the',\n",
       "  'world',\n",
       "  ',',\n",
       "  'no',\n",
       "  'one',\n",
       "  'will',\n",
       "  'respect',\n",
       "  'us',\n",
       "  '.'],\n",
       " ['only', 'strength', 'respects', 'strength', '.'],\n",
       " ['we',\n",
       "  'must',\n",
       "  'be',\n",
       "  'strong',\n",
       "  'not',\n",
       "  'only',\n",
       "  'as',\n",
       "  'a',\n",
       "  'military',\n",
       "  'power',\n",
       "  'but',\n",
       "  'also',\n",
       "  'as',\n",
       "  'an',\n",
       "  'economic',\n",
       "  'power',\n",
       "  '.'],\n",
       " ['both', 'must', 'go', 'hand-in-hand', '.'],\n",
       " ['my',\n",
       "  'good',\n",
       "  'fortune',\n",
       "  'was',\n",
       "  'to',\n",
       "  'have',\n",
       "  'worked',\n",
       "  'with',\n",
       "  'three',\n",
       "  'great',\n",
       "  'minds',\n",
       "  '.'],\n",
       " ['dr.', 'vikram', 'sarabhai', 'of', 'the', 'dept', '.'],\n",
       " ['of',\n",
       "  'space',\n",
       "  ',',\n",
       "  'professor',\n",
       "  'satish',\n",
       "  'dhawan',\n",
       "  ',',\n",
       "  'who',\n",
       "  'succeeded',\n",
       "  'him',\n",
       "  'and',\n",
       "  'dr.',\n",
       "  'brahm',\n",
       "  'prakash',\n",
       "  ',',\n",
       "  'father',\n",
       "  'of',\n",
       "  'nuclear',\n",
       "  'material',\n",
       "  '.'],\n",
       " ['i',\n",
       "  'was',\n",
       "  'lucky',\n",
       "  'to',\n",
       "  'have',\n",
       "  'worked',\n",
       "  'with',\n",
       "  'all',\n",
       "  'three',\n",
       "  'of',\n",
       "  'them',\n",
       "  'closely',\n",
       "  'and',\n",
       "  'consider',\n",
       "  'this',\n",
       "  'the',\n",
       "  'great',\n",
       "  'opportunity',\n",
       "  'of',\n",
       "  'my',\n",
       "  'life',\n",
       "  '.'],\n",
       " ['i', 'see', 'four', 'milestones', 'in', 'my', 'career']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "7YCgUHWQuLcz",
    "outputId": "dca4c3ef-6514-4548-c036-aed4e19f5ffe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91956\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oqk8FExxqW3E"
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "FcgeYvXpvgBm",
    "outputId": "87e40fee-7ca0-4b75-cc8f-a8fd26ebb6a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91956\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pUufnhSoqnMw",
    "outputId": "8ee61598-c7f1-4e6d-93dd-d0f2527f0ac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=29, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "#here we need to convert each and every word to vectors with the help of word2vec    \n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=2)#we can change the value here.if the word is present less than twice,remove\n",
    "#word2vec is used mainly for huge amount of data.eg:we may apply this on entire doc of wikepedia\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "MXoORipExRtl",
    "outputId": "3b83115b-474f-43d2-d776-d89df5536091"
   },
   "outputs": [],
   "source": [
    "#words = model.wv.vocab\n",
    "#words\n",
    "#here words will be represented in alphabetic orders.each word will be represented as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "_RDU5SkGq3ot",
    "outputId": "bfb30411-f2be-443e-e912-1b99d2c2e985"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.2470402e-03,  9.3104085e-03, -1.9035455e-04, -1.9678909e-03,\n",
       "        4.6107038e-03, -4.0970482e-03,  2.7569898e-03,  6.9491598e-03,\n",
       "        6.0659950e-03, -7.5209257e-03,  9.3941428e-03,  4.6645785e-03,\n",
       "        3.9729690e-03, -6.2513947e-03,  8.4632728e-03, -2.1494904e-03,\n",
       "        8.8308081e-03, -5.3665224e-03, -8.1366980e-03,  6.8107611e-03,\n",
       "        1.6734540e-03, -2.1905121e-03,  9.5245503e-03,  9.4974982e-03,\n",
       "       -9.7854072e-03,  2.5109882e-03,  6.1551095e-03,  3.8626830e-03,\n",
       "        2.0276017e-03,  4.2483912e-04,  6.7750097e-04, -3.8248522e-03,\n",
       "       -7.1342522e-03, -2.0816177e-03,  3.9217160e-03,  8.8225566e-03,\n",
       "        9.2622126e-03, -5.9755617e-03, -9.4068600e-03,  9.7596208e-03,\n",
       "        3.4338485e-03,  5.1686266e-03,  6.2874253e-03, -2.8009326e-03,\n",
       "        7.3261638e-03,  2.8237968e-03,  2.8704628e-03, -2.3871814e-03,\n",
       "       -3.1312704e-03, -2.3685780e-03,  4.2830193e-03,  7.6171578e-05,\n",
       "       -9.5888041e-03, -9.6761463e-03, -6.1491285e-03, -1.3418916e-04,\n",
       "        2.0023400e-03,  9.4349608e-03,  5.5920654e-03, -4.2967470e-03,\n",
       "        2.6875074e-04,  4.9580284e-03,  7.7168918e-03, -1.1418007e-03,\n",
       "        4.3112319e-03, -5.8084754e-03, -8.1237534e-04,  8.1023183e-03,\n",
       "       -2.3731852e-03, -9.6672000e-03,  5.7765474e-03, -3.9252904e-03,\n",
       "       -1.2171328e-03,  9.9904612e-03, -2.2583348e-03, -4.7633834e-03,\n",
       "       -5.3234603e-03,  6.9806427e-03, -5.7191066e-03,  2.1177260e-03,\n",
       "       -5.2607311e-03,  6.1210375e-03,  4.3624546e-03,  2.6122951e-03,\n",
       "       -1.4900486e-03, -2.7447680e-03,  9.0019563e-03,  5.2146213e-03,\n",
       "       -2.1665778e-03, -9.4684344e-03, -7.4244295e-03, -1.0633167e-03,\n",
       "       -7.8949908e-04, -2.5591084e-03,  9.6967360e-03, -4.6107543e-04,\n",
       "        5.8837999e-03, -7.4433978e-03, -2.5018277e-03, -5.5518956e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Finding Word Vectors\n",
    "vector = model.wv['vision']\n",
    "vector\n",
    "#each word will be represented as a vector of 32 or more dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "VwcowUm4y0gs",
    "outputId": "3ff7fa8f-c003-416f-95f0-8cc5a12310d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dr.', 0.3039070665836334),\n",
       " ('great', 0.17799635231494904),\n",
       " ('years', 0.16379284858703613),\n",
       " ('?', 0.16260458528995514),\n",
       " ('india', 0.1460980921983719),\n",
       " ('developed', 0.11138284206390381),\n",
       " ('minds', 0.09419557452201843),\n",
       " ('strength', 0.07514531910419464),\n",
       " ('three', 0.05126914009451866),\n",
       " ('vision', 0.04206670820713043)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most similar words\n",
    "similar = model.wv.most_similar('nation')\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlp2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
